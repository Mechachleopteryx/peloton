// Copyright (c) 2019 Uber Technologies, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package aurorabridge

import (
	"context"
	"fmt"
	"sync"
	"sync/atomic"

	"github.com/uber/peloton/.gen/peloton/api/v1alpha/job/stateless"
	statelesssvc "github.com/uber/peloton/.gen/peloton/api/v1alpha/job/stateless/svc"
	"github.com/uber/peloton/.gen/peloton/api/v1alpha/peloton"
	"github.com/uber/peloton/.gen/peloton/api/v1alpha/pod"
	podsvc "github.com/uber/peloton/.gen/peloton/api/v1alpha/pod/svc"
	pbquery "github.com/uber/peloton/.gen/peloton/api/v1alpha/query"
	"github.com/uber/peloton/.gen/thrift/aurora/api"
	"github.com/uber/peloton/common/async"
	"github.com/uber/peloton/util"

	"github.com/uber/peloton/aurorabridge/atop"
	"github.com/uber/peloton/aurorabridge/label"
	"github.com/uber/peloton/aurorabridge/opaquedata"
	"github.com/uber/peloton/aurorabridge/ptoa"

	"github.com/pkg/errors"
	log "github.com/sirupsen/logrus"
	"github.com/uber-go/tally"
	"go.uber.org/thriftrw/ptr"
	"go.uber.org/yarpc/yarpcerrors"
)

var errUnimplemented = errors.New("rpc is unimplemented")

// ServiceHandler implements a partial Aurora API. Various unneeded methods have
// been left intentionally unimplemented.
type ServiceHandler struct {
	config        ServiceHandlerConfig
	metrics       *Metrics
	jobClient     statelesssvc.JobServiceYARPCClient
	podClient     podsvc.PodServiceYARPCClient
	respoolLoader RespoolLoader
}

// NewServiceHandler creates a new ServiceHandler.
func NewServiceHandler(
	config ServiceHandlerConfig,
	parent tally.Scope,
	jobClient statelesssvc.JobServiceYARPCClient,
	podClient podsvc.PodServiceYARPCClient,
	respoolLoader RespoolLoader,
) *ServiceHandler {
	config.normalize()
	return &ServiceHandler{
		config:        config,
		metrics:       NewMetrics(parent.SubScope("aurorabridge").SubScope("api")),
		jobClient:     jobClient,
		podClient:     podClient,
		respoolLoader: respoolLoader,
	}
}

// GetJobSummary returns a summary of jobs, optionally only those owned by a specific role.
func (h *ServiceHandler) GetJobSummary(
	ctx context.Context,
	role *string,
) (*api.Response, error) {

	result, err := h.getJobSummary(ctx, role)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"role": role,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetJobSummary error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) getJobSummary(
	ctx context.Context,
	role *string,
) (*api.Result, *auroraError) {

	var jobSummaries []*api.JobSummary

	query := &api.TaskQuery{Role: role}

	jobIDs, err := h.getJobIDsFromTaskQuery(ctx, query)
	if err != nil {
		return nil, auroraErrorf("get job ids from task query: %s", err)
	}

	for _, jobID := range jobIDs {
		jobInfo, err := h.getJobInfo(ctx, jobID)
		if err != nil {
			return nil, auroraErrorf("get job info for job id %q: %s",
				jobID.GetValue(), err)
		}

		// In Aurora, JobSummary.JobConfiguration.TaskConfig
		// is generated using latest "active" task. Reference:
		// https://github.com/apache/aurora/blob/master/src/main/java/org/apache/aurora/scheduler/base/Tasks.java#L133
		// but use JobInfo.JobSpec.DefaultSpec here to simplify
		// the querying logic.
		// TODO(kevinxu): Need to match Aurora's behavior?
		// TODO(kevinxu): Need to inspect InstanceSpec as well?
		podSpec := jobInfo.GetSpec().GetDefaultSpec()

		s, err := ptoa.NewJobSummary(jobInfo, podSpec)
		if err != nil {
			return nil, auroraErrorf("new job summary: %s", err)
		}

		jobSummaries = append(jobSummaries, s)
	}

	return &api.Result{
		JobSummaryResult: &api.JobSummaryResult{
			Summaries: jobSummaries,
		},
	}, nil
}

// GetTasksWithoutConfigs is the same as getTasksStatus but without the TaskConfig.ExecutorConfig
// data set.
func (h *ServiceHandler) GetTasksWithoutConfigs(
	ctx context.Context,
	query *api.TaskQuery,
) (*api.Response, error) {

	result, err := h.getTasksWithoutConfigs(ctx, query)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"query": query,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetTasksWithoutConfigs error")
	}
	return newResponse(result, err), nil
}

// getScheduledTaskResult represents output
// value returned on channel generated by
// getTasksWithoutConfigs and error if occured
type getScheduledTaskResult struct {
	task *api.ScheduledTask
	err  error
}

func (h *ServiceHandler) getTasksWithoutConfigs(
	ctx context.Context,
	query *api.TaskQuery,
) (*api.Result, *auroraError) {

	var scheduledTasks []*api.ScheduledTask
	var podStates []pod.PodState

	for s := range query.GetStatuses() {
		p, err := atop.NewPodState(s)
		if err != nil {
			return nil, auroraErrorf("new pod state: %s", err)
		}
		podStates = append(podStates, p)
	}

	jobIDs, err := h.getJobIDsFromTaskQuery(ctx, query)
	if err != nil {
		return nil, auroraErrorf("get job ids from task query: %s", err)
	}

	// TODO(kevinxu): Factor out to seperate function.
	for _, jobID := range jobIDs {
		jobInfo, err := h.getJobInfo(ctx, jobID)
		if err != nil {
			return nil, auroraErrorf("get job info for job id %q: %s",
				jobID.GetValue(), err)
		}

		// TODO(kevinxu): QueryPods api only returns pods from latest run,
		// while aurora returns tasks from all previous runs. Do we need
		// to make it consistent with aurora's behavior?
		pods, err := h.queryPods(
			ctx,
			jobID,
			podStates,
			jobInfo.GetSpec().GetInstanceCount(),
		)
		if err != nil {
			return nil, auroraErrorf(
				"query pods for job id %q with pod states %q: %s",
				jobID.GetValue(), podStates, err)
		}

		tasks, err := h.getScheduledTasks(ctx, jobInfo, pods)
		if err != nil {
			return nil, auroraErrorf("get tasks without configs: %s", err)
		}
		scheduledTasks = append(scheduledTasks, tasks...)
	}

	return &api.Result{
		ScheduleStatusResult: &api.ScheduleStatusResult{
			Tasks: scheduledTasks,
		},
	}, nil
}

// getScheduledTasks generates a list of Aurora ScheduledTask in a worker
// pool.
func (h *ServiceHandler) getScheduledTasks(
	ctx context.Context,
	jobInfo *stateless.JobInfo,
	podInfos []*pod.PodInfo,
) ([]*api.ScheduledTask, error) {
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	podc := make(chan *pod.PodInfo)
	resultc := make(chan *getScheduledTaskResult)

	var wg sync.WaitGroup
	for w := 0; w < h.config.GetTasksWithoutConfigsWorkers; w++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			for {
				select {
				case p, ok := <-podc:
					if !ok {
						return
					}
					t, err := h.getScheduledTask(ctx, jobInfo, p)
					select {
					case resultc <- &getScheduledTaskResult{t, err}:
					case <-ctx.Done():
						return
					}
				case <-ctx.Done():
					return
				}
			}
		}()
	}

	go func() {
		for _, p := range podInfos {
			select {
			case podc <- p:
			case <-ctx.Done():
				return
			}
		}
		close(podc)
		wg.Wait()
		close(resultc)
	}()

	var tasks []*api.ScheduledTask
	for {
		select {
		case r, ok := <-resultc:
			if !ok {
				return tasks, nil
			}

			if r.err != nil {
				return nil, r.err
			}
			tasks = append(tasks, r.task)
		case <-ctx.Done():
			return nil, ctx.Err()
		}
	}
}

// getScheduledTask calls getPodEvents and returns Aurora ScheduledTask.
func (h *ServiceHandler) getScheduledTask(
	ctx context.Context,
	jobInfo *stateless.JobInfo,
	podInfo *pod.PodInfo,
) (*api.ScheduledTask, error) {
	n := podInfo.GetSpec().GetPodName()
	podEvents, err := h.getPodEvents(ctx, n)
	if err != nil {
		return nil, fmt.Errorf("get pod events for pod %q: %s",
			n.GetValue(), err)
	}
	t, err := ptoa.NewScheduledTask(jobInfo, podInfo, podEvents)
	if err != nil {
		return nil, fmt.Errorf("new scheduled task: %s", err)
	}
	return t, nil
}

// GetConfigSummary fetches the configuration summary of active tasks for the specified job.
func (h *ServiceHandler) GetConfigSummary(
	ctx context.Context,
	job *api.JobKey) (*api.Response, error) {

	result, err := h.getConfigSummary(ctx, job)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"job": job,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetConfigSummary error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) getConfigSummary(
	ctx context.Context,
	jobKey *api.JobKey,
) (*api.Result, *auroraError) {
	jobID, err := h.getJobID(
		ctx,
		jobKey)
	if err != nil {
		return nil, auroraErrorf("unable to get jobID from jobKey: %s", err)
	}

	jobInfo, err := h.getJobInfo(ctx, jobID)
	if err != nil {
		return nil, auroraErrorf("unable to get jobInfo from jobID: %s", err)
	}

	podInfos, err := h.queryPods(
		ctx,
		jobID,
		nil,
		jobInfo.GetSpec().GetInstanceCount())
	if err != nil {
		return nil, auroraErrorf("unable to query pods using jobID: %s", err)
	}

	configSummary, err := ptoa.NewConfigSummary(
		jobInfo,
		podInfos)
	if err != nil {
		return nil, auroraErrorf("unable to get config summary from podInfos: %s", err)
	}

	return &api.Result{
		ConfigSummaryResult: &api.ConfigSummaryResult{
			Summary: configSummary,
		},
	}, nil
}

// GetJobs fetches the status of jobs. ownerRole is optional, in which case all jobs are returned.
func (h *ServiceHandler) GetJobs(
	ctx context.Context,
	ownerRole *string,
) (*api.Response, error) {

	result, err := h.getJobs(ctx, ownerRole)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"ownerRole": ownerRole,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetJobs error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) getJobs(
	ctx context.Context,
	ownerRole *string,
) (*api.Result, *auroraError) {

	var configs []*api.JobConfiguration

	query := &api.TaskQuery{Role: ownerRole}

	jobIDs, err := h.getJobIDsFromTaskQuery(ctx, query)
	if err != nil {
		return nil, auroraErrorf("get job ids from task query: %s", err)
	}

	for _, jobID := range jobIDs {
		jobInfo, err := h.getJobInfo(ctx, jobID)
		if err != nil {
			return nil, auroraErrorf("get job info for job id %q: %s",
				jobID.GetValue(), err)
		}

		// In Aurora, JobConfiguration.TaskConfig
		// is generated using latest "active" task. Reference:
		// https://github.com/apache/aurora/blob/master/src/main/java/org/apache/aurora/scheduler/base/Tasks.java#L133
		// but use JobInfo.JobSpec.DefaultSpec here to simplify
		// the querying logic.
		// TODO(kevinxu): Need to match Aurora's behavior?
		// TODO(kevinxu): Need to inspect InstanceSpec as well?
		podSpec := jobInfo.GetSpec().GetDefaultSpec()

		c, err := ptoa.NewJobConfiguration(jobInfo, podSpec)
		if err != nil {
			return nil, auroraErrorf("new job configuration: %s", err)
		}

		configs = append(configs, c)
	}

	return &api.Result{
		GetJobsResult: &api.GetJobsResult{
			Configs: configs,
		},
	}, nil
}

// GetJobUpdateSummaries gets job update summaries.
// This is the sequence in which jobUpdateQuery filters updates.
// - Get JobIDs using job key role and filter their updates
// - Get JobIDs using job key and filter their updates
// - If only update statuses are provided then filter all job updates
func (h *ServiceHandler) GetJobUpdateSummaries(
	ctx context.Context,
	jobUpdateQuery *api.JobUpdateQuery,
) (*api.Response, error) {

	jobUpdateDetails, err := h.getJobUpdateDetails(
		ctx,
		jobUpdateQuery,
		true, /* summary only */
	)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"key": jobUpdateQuery,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetJobUpdateSummaries error")
	}

	var jobUpdateSummaries []*api.JobUpdateSummary
	for _, jobUpdateDetail := range jobUpdateDetails {
		jobUpdateSummaries = append(jobUpdateSummaries, jobUpdateDetail.GetUpdate().GetSummary())
	}

	return newResponse(&api.Result{
		GetJobUpdateSummariesResult: &api.GetJobUpdateSummariesResult{
			UpdateSummaries: jobUpdateSummaries,
		},
	}, err), nil
}

// GetJobUpdateDetails gets job update details.
// jobUpdateKey is marked to be deprecated from Aurora, and not used Aggregator
// It will be ignored to get job update details
func (h *ServiceHandler) GetJobUpdateDetails(
	ctx context.Context,
	key *api.JobUpdateKey,
	query *api.JobUpdateQuery) (*api.Response, error) {

	return nil, errUnimplemented
}

// GetJobUpdateDiff gets the diff between client (desired) and server (current) job states.
// TaskConfig is not set in GetJobUpdateDiffResult, since caller is not using it
// and fetching previous podspec is expensive
func (h *ServiceHandler) GetJobUpdateDiff(
	ctx context.Context,
	request *api.JobUpdateRequest) (*api.Response, error) {

	result, err := h.getJobUpdateDiff(ctx, request)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"request": request,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("GetJobUpdateDiff error")
	}

	return newResponse(result, err), nil
}

func (h *ServiceHandler) getJobUpdateDiff(
	ctx context.Context,
	request *api.JobUpdateRequest,
) (*api.Result, *auroraError) {

	respoolID, err := h.respoolLoader.Load(ctx)
	if err != nil {
		return nil, auroraErrorf("load respool: %s", err)
	}

	jobKey := request.GetTaskConfig().GetJob()
	jobID, err := h.getJobID(ctx, jobKey)
	if err != nil {
		return nil, auroraErrorf("get jobID: %s", err)
	}

	jobSummary, err := h.getJobInfoSummary(ctx, jobID)
	if err != nil {
		return nil, auroraErrorf("get job summary: %s", err)
	}

	jobSpec, err := atop.NewJobSpecFromJobUpdateRequest(request, respoolID)
	if err != nil {
		return nil, auroraErrorf("new job spec: %s", err)
	}

	resp, err := h.jobClient.GetReplaceJobDiff(
		ctx,
		&statelesssvc.GetReplaceJobDiffRequest{
			JobId:   jobID,
			Version: jobSummary.GetStatus().GetVersion(),
			Spec:    jobSpec,
		})
	if err != nil {
		return nil, auroraErrorf("get replace job diff: %s", err)
	}

	return &api.Result{
		GetJobUpdateDiffResult: &api.GetJobUpdateDiffResult{
			Add: []*api.ConfigGroup{{
				Instances: ptoa.NewRange(resp.GetInstancesAdded()),
			}},
			Update: []*api.ConfigGroup{{
				Instances: ptoa.NewRange(resp.GetInstancesUpdated()),
			}},
			Remove: []*api.ConfigGroup{{
				Instances: ptoa.NewRange(resp.GetInstancesRemoved()),
			}},
			Unchanged: []*api.ConfigGroup{{
				Instances: ptoa.NewRange(resp.GetInstancesUnchanged()),
			}},
		},
	}, nil
}

// GetTierConfigs is a no-op. It is only used to determine liveness of the scheduler.
func (h *ServiceHandler) GetTierConfigs(
	ctx context.Context) (*api.Response, error) {
	return nil, errUnimplemented
}

// KillTasks initiates a kill on tasks.
func (h *ServiceHandler) KillTasks(
	ctx context.Context,
	job *api.JobKey,
	instances map[int32]struct{},
	message *string,
) (*api.Response, error) {

	result, err := h.killTasks(ctx, job, instances, message)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"job":       job,
				"instances": instances,
				"message":   message,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("KillTasks error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) killTasks(
	ctx context.Context,
	job *api.JobKey,
	instances map[int32]struct{},
	message *string,
) (*api.Result, *auroraError) {

	id, err := h.getJobID(ctx, job)
	if err != nil {
		return nil, auroraErrorf("get job id: %s", err)
	}
	summary, err := h.getJobInfoSummary(ctx, id)
	if err != nil {
		return nil, auroraErrorf("get job info summary: %s", err)
	}

	stopAll := false
	if uint32(len(instances)) == summary.GetInstanceCount() {
		// Sanity check to make sure we don't stop everything if instances
		// are out of bounds.
		low, high := instanceBounds(instances)
		if low == 0 && high == int32(len(instances)-1) {
			stopAll = true
		}
	}

	if stopAll {
		// If all instances are specified, issue a single StopJob instead of
		// multiple StopPods for performance reasons.
		req := &statelesssvc.StopJobRequest{
			JobId:   id,
			Version: summary.GetStatus().GetVersion(),
		}
		if _, err := h.jobClient.StopJob(ctx, req); err != nil {
			return nil, auroraErrorf("stop job: %s", err)
		}
	} else {
		if err := h.stopPodsConcurrently(ctx, id, instances); err != nil {
			return nil, auroraErrorf("stop pods in parallel: %s", err)
		}
	}
	return &api.Result{}, nil
}

func (h *ServiceHandler) stopPodsConcurrently(
	ctx context.Context,
	id *peloton.JobID,
	instances map[int32]struct{},
) error {

	instc := make(chan int32)
	errc := make(chan error)

	// Ensure all channel sends/recvs have a release valve if we encounter
	// an early error.
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	var wg sync.WaitGroup
	for w := 0; w < h.config.StopPodWorkers; w++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			for {
				select {
				case i, ok := <-instc:
					if !ok {
						return
					}
					name := util.CreatePelotonTaskID(id.GetValue(), uint32(i))
					req := &podsvc.StopPodRequest{
						PodName: &peloton.PodName{Value: name},
					}
					if _, err := h.podClient.StopPod(ctx, req); err != nil {
						select {
						case errc <- fmt.Errorf("stop pod %d: %s", i, err):
						case <-ctx.Done():
							return
						}
					}
				case <-ctx.Done():
					return
				}
			}
		}()
	}

	go func() {
		for i := range instances {
			select {
			case instc <- i:
			case <-ctx.Done():
				return
			}
		}
		close(instc) // Signal to workers there are no more inputs.
		wg.Wait()    // Wait for workers to finish in-progress work.
		close(errc)  // Signal to consumer that work is finished.
	}()

	select {
	case err := <-errc:
		return err
	case <-ctx.Done():
		return ctx.Err()
	}
}

// instanceBounds returns the lowest and highest instance id of
// instances. If instances is empty, returns -1.
func instanceBounds(instances map[int32]struct{}) (low, high int32) {
	if len(instances) == 0 {
		return -1, -1
	}
	for i := range instances {
		if i < low {
			low = i
		}
		if i > high {
			high = i
		}
	}
	return low, high
}

// StartJobUpdate starts update of the existing service job.
func (h *ServiceHandler) StartJobUpdate(
	ctx context.Context,
	request *api.JobUpdateRequest,
	message *string,
) (*api.Response, error) {

	result, err := h.startJobUpdate(ctx, request, message)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"request": request,
				"message": message,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("StartJobUpdate error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) startJobUpdate(
	ctx context.Context,
	request *api.JobUpdateRequest,
	message *string,
) (*api.Result, *auroraError) {

	respoolID, err := h.respoolLoader.Load(ctx)
	if err != nil {
		return nil, auroraErrorf("load respool: %s", err)
	}

	jobKey := request.GetTaskConfig().GetJob()

	jobSpec, err := atop.NewJobSpecFromJobUpdateRequest(request, respoolID)
	if err != nil {
		return nil, auroraErrorf("new job spec: %s", err)
	}

	// TODO(codyg): We'll use the new job's entity version as the update id.
	// Not sure if this will work.
	var newVersion *peloton.EntityVersion

	id, err := h.getJobID(ctx, jobKey)
	if err != nil {
		if yarpcerrors.IsNotFound(err) {
			// Job does not exist. Create it.
			req := &statelesssvc.CreateJobRequest{
				Spec: jobSpec,
			}
			resp, err := h.jobClient.CreateJob(ctx, req)
			if err != nil {
				if yarpcerrors.IsAlreadyExists(err) {
					// Upgrade conflict.
					return nil, auroraErrorf(
						"create job: %s", err).
						code(api.ResponseCodeInvalidRequest)
				}
				return nil, auroraErrorf("create job: %s", err)
			}
			newVersion = resp.GetVersion()
		} else {
			return nil, auroraErrorf("get job id: %s", err)
		}
	} else {
		// Job already exists. Replace it.
		v, err := h.getCurrentJobVersion(ctx, id)
		if err != nil {
			return nil, auroraErrorf("get current job version: %s", err)
		}

		d := &opaquedata.Data{}
		if request.GetSettings().GetBlockIfNoPulsesAfterMs() > 0 {
			d.AppendUpdateAction(opaquedata.StartPulsed)
		}
		od, err := d.Serialize()
		if err != nil {
			return nil, auroraErrorf("serialize opaque data: %s", err)
		}

		req := &statelesssvc.ReplaceJobRequest{
			JobId:      id,
			Spec:       jobSpec,
			UpdateSpec: atop.NewUpdateSpec(request.GetSettings()),
			Version:    v,
			OpaqueData: od,
		}
		resp, err := h.jobClient.ReplaceJob(ctx, req)
		if err != nil {
			if yarpcerrors.IsAborted(err) {
				// Upgrade conflict.
				return nil, auroraErrorf(
					"replace job: %s", err).
					code(api.ResponseCodeInvalidRequest)
			}
			return nil, auroraErrorf("replace job: %s", err)
		}
		newVersion = resp.GetVersion()
	}

	return &api.Result{
		StartJobUpdateResult: &api.StartJobUpdateResult{
			Key: &api.JobUpdateKey{
				Job: jobKey,
				ID:  ptr.String(newVersion.String()),
			},
			UpdateSummary: nil, // TODO(codyg): Should we set this?
		},
	}, nil
}

// PauseJobUpdate pauses the specified job update. Can be resumed by resumeUpdate call.
func (h *ServiceHandler) PauseJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Response, error) {

	result, err := h.pauseJobUpdate(ctx, key, message)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"key":     key,
				"message": message,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("PauseJobUpdate error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) pauseJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Result, *auroraError) {

	id, err := h.getJobID(ctx, key.GetJob())
	if err != nil {
		return nil, auroraErrorf("get job id: %s", err)
	}
	v, err := h.getCurrentJobVersion(ctx, id)
	if err != nil {
		return nil, auroraErrorf("get current job version: %s", err)
	}
	req := &statelesssvc.PauseJobWorkflowRequest{
		JobId:   id,
		Version: v,
	}
	if _, err := h.jobClient.PauseJobWorkflow(ctx, req); err != nil {
		return nil, auroraErrorf("pause job workflow: %s", err)
	}
	return &api.Result{}, nil
}

// ResumeJobUpdate resumes progress of a previously paused job update.
func (h *ServiceHandler) ResumeJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Response, error) {

	result, err := h.resumeJobUpdate(ctx, key, message)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"key":     key,
				"message": message,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("ResumeJobUpdate error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) resumeJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Result, *auroraError) {

	id, err := h.getJobID(ctx, key.GetJob())
	if err != nil {
		return nil, auroraErrorf("get job id: %s", err)
	}
	v, err := h.getCurrentJobVersion(ctx, id)
	if err != nil {
		return nil, auroraErrorf("get current job version: %s", err)
	}
	req := &statelesssvc.ResumeJobWorkflowRequest{
		JobId:   id,
		Version: v,
	}
	if _, err := h.jobClient.ResumeJobWorkflow(ctx, req); err != nil {
		return nil, auroraErrorf("resume job workflow: %s", err)
	}
	return &api.Result{}, nil
}

// AbortJobUpdate permanently aborts the job update. Does not remove the update history.
func (h *ServiceHandler) AbortJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Response, error) {

	result, err := h.abortJobUpdate(ctx, key, message)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"key":     key,
				"message": message,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("AbortJobUpdate error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) abortJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string,
) (*api.Result, *auroraError) {

	id, err := h.getJobID(ctx, key.GetJob())
	if err != nil {
		return nil, auroraErrorf("get job id: %s", err)
	}
	v, err := h.getCurrentJobVersion(ctx, id)
	if err != nil {
		return nil, auroraErrorf("get current job version: %s", err)
	}
	req := &statelesssvc.AbortJobWorkflowRequest{
		JobId:   id,
		Version: v,
	}
	if _, err := h.jobClient.AbortJobWorkflow(ctx, req); err != nil {
		return nil, auroraErrorf("abort job workflow: %s", err)
	}
	return &api.Result{}, nil
}

// RollbackJobUpdate rollbacks the specified active job update to the initial state.
func (h *ServiceHandler) RollbackJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
	message *string) (*api.Response, error) {

	return nil, errUnimplemented
}

// PulseJobUpdate allows progress of the job update in case blockIfNoPulsesAfterMs is specified in
// JobUpdateSettings. Unblocks progress if the update was previously blocked.
// Responds with ResponseCode.INVALID_REQUEST in case an unknown update key is specified.
func (h *ServiceHandler) PulseJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
) (*api.Response, error) {

	result, err := h.pulseJobUpdate(ctx, key)
	if err != nil {
		log.WithFields(log.Fields{
			"params": log.Fields{
				"key": key,
			},
			"code":  err.responseCode,
			"error": err.msg,
		}).Error("PulseJobUpdate error")
	}
	return newResponse(result, err), nil
}

func (h *ServiceHandler) pulseJobUpdate(
	ctx context.Context,
	key *api.JobUpdateKey,
) (*api.Result, *auroraError) {

	id, err := h.getJobID(ctx, key.GetJob())
	if err != nil {
		aerr := auroraErrorf("get job id: %s", err)
		if yarpcerrors.IsNotFound(err) {
			// Unknown update.
			// TODO(codyg): We should support some form of update ID.
			aerr.code(api.ResponseCodeInvalidRequest)
		}
		return nil, aerr
	}

	w, err := h.getWorkflowInfo(ctx, id)
	if err != nil {
		return nil, auroraErrorf("get workflow info: %s", err)
	}

	d, err := opaquedata.Deserialize(w.GetOpaqueData())
	if err != nil {
		return nil, auroraErrorf("deserialize opaque data: %s", err)
	}

	status, err := ptoa.NewJobUpdateStatus(w.GetStatus().GetState(), d)
	if err != nil {
		return nil, auroraErrorf("new job update status: %s", err)
	}

	// Only resume if we're in an AWAITING_PULSE state. Else, pulseJobUpdate is
	// a no-op.
	if status == api.JobUpdateStatusRollForwardAwaitingPulse ||
		status == api.JobUpdateStatusRollBackAwaitingPulse {

		d.AppendUpdateAction(opaquedata.Pulse)
		od, err := d.Serialize()
		if err != nil {
			return nil, auroraErrorf("serialize opaque data: %s", err)
		}

		req := &statelesssvc.ResumeJobWorkflowRequest{
			JobId:      id,
			Version:    w.GetStatus().GetVersion(),
			OpaqueData: od,
		}
		if _, err := h.jobClient.ResumeJobWorkflow(ctx, req); err != nil {
			return nil, auroraErrorf("resume job workflow: %s", err)
		}
	}

	return &api.Result{
		PulseJobUpdateResult: &api.PulseJobUpdateResult{
			Status: api.JobUpdatePulseStatusOk.Ptr(),
		},
	}, nil
}

// jobUpdateResult represents output value returned on channel
// to get job update detail if not filtered
// and error if occured
type jobUpdateResult struct {
	detail     *api.JobUpdateDetails
	isFiltered bool /* set true, if job update state not in expected update statuses */
	err        error
}

// getJobUpdateDetails gets job detils concurrently
func (h *ServiceHandler) getJobUpdateDetails(
	ctx context.Context,
	jobUpdateQuery *api.JobUpdateQuery,
	summaryOnly bool,
) ([]*api.JobUpdateDetails, *auroraError) {
	var jobUpdateDetails []*api.JobUpdateDetails

	jobSummaries, err := h.getJobSummariesFromJobUpdateQuery(
		ctx,
		jobUpdateQuery)
	if err != nil {
		return jobUpdateDetails, nil
	}

	outchan := make(chan *jobUpdateResult)
	pool := async.NewPool(async.PoolOptions{
		MaxWorkers: h.config.GetJobUpdateWorkers,
	}, nil)
	pool.Start()

	getJobDetailsErrors := uint32(0)
	var wg sync.WaitGroup
	for _, jobSummary := range jobSummaries {
		wg.Add(1)
		go func(jobSummary *stateless.JobSummary) {
			defer wg.Done()
			pool.Enqueue(async.JobFunc(func(context.Context) {
				// stop fetching more getJobUpdateDetails on first error.
				if atomic.LoadUint32(&getJobDetailsErrors) > 0 {
					outchan <- &jobUpdateResult{}
					return
				}
				jobUpdateDetail, isFiltered, err := h.getJobUpdateDetail(
					ctx,
					jobUpdateQuery,
					jobSummary,
					summaryOnly)

				outchan <- &jobUpdateResult{
					detail:     jobUpdateDetail,
					isFiltered: isFiltered,
					err:        err,
				}
			}))
		}(jobSummary)
	}
	wg.Wait()

	for i := 0; i < len(jobSummaries); i++ {
		result := <-outchan

		if result.err != nil {
			log.WithError(result.err).Error("failed to get jobUpdateDetail from jobSummary")
			atomic.AddUint32(&getJobDetailsErrors, 1)
			continue
		}

		// isFiltered set to true, if workflow is in INITIALIZED state
		// or workflow state is filtered on provided update query
		if result.isFiltered == true {
			continue
		}

		jobUpdateDetails = append(jobUpdateDetails, result.detail)
	}

	if getJobDetailsErrors > 0 {
		return nil, auroraErrorf("failed to get jobUpdateDetails for %d jobs", getJobDetailsErrors)
	}

	return jobUpdateDetails, nil
}

// getJobUpdateDetail get details of most recent workflow of the job
func (h *ServiceHandler) getJobUpdateDetail(
	ctx context.Context,
	jobUpdateQuery *api.JobUpdateQuery,
	jobSummary *stateless.JobSummary,
	summaryOnly bool,
) (*api.JobUpdateDetails, bool, error) {

	// Get job update
	resp, err := h.jobClient.GetJob(
		ctx,
		&statelesssvc.GetJobRequest{
			JobId: jobSummary.GetJobId(),
		})
	if err != nil {
		return nil, false, fmt.Errorf("getJobUpdate failed: %s", err)
	}

	// INITIALIZED workflows are ignored to take action upon because Aurora
	// does not support this state
	// Assumption is that jobmgr's goal state engine will process these
	// workflows and take appropriate action to make progress on
	// workflow operations
	if resp.GetWorkflowInfo().GetStatus().GetState() ==
		stateless.WorkflowState_WORKFLOW_STATE_INITIALIZED {
		return nil, true, nil /* update is filtered */
	}

	// Filter by job update state
	ok, err := isUpdateInfoInStatuses(
		resp.GetWorkflowInfo(),
		jobUpdateQuery.GetUpdateStatuses())
	if err != nil {
		return nil, false, err
	}
	if !ok {
		return nil, true, nil /* update is filtered */
	}

	// Get Job Update Summary
	jobKey, err := ptoa.NewJobKey(jobSummary.GetName())
	if err != nil {
		return nil, false, err
	}

	jobUpdateSummary, err := ptoa.NewJobUpdateSummary(
		jobKey,
		resp.GetWorkflowInfo())
	if err != nil {
		return nil, false, err
	}

	// Get job update details if summaryOnly is set false
	// TODO:
	// 1. instance workflow events
	// 2. job update events
	// 3. update spec
	// 4. job specs
	if !summaryOnly {
	}

	return &api.JobUpdateDetails{
		Update: &api.JobUpdate{
			Summary:      jobUpdateSummary,
			Instructions: nil,
		},
		UpdateEvents:   nil,
		InstanceEvents: nil,
	}, false, nil
}

// getJobSummariesFromJobUpdateQuery queries peloton jobs based on
// Aurora's JobUpdateQuery.
func (h *ServiceHandler) getJobSummariesFromJobUpdateQuery(
	ctx context.Context,
	q *api.JobUpdateQuery,
) ([]*stateless.JobSummary, error) {

	if q.IsSetJobKey() {
		return h.getJobSummaries(ctx, q.GetJobKey())
	}
	return h.queryJobSummaries(ctx, q.GetRole(), "", "")
}

// isUpdateInfoInStatuses checks if job update state is present
// in expected list of job update states
func isUpdateInfoInStatuses(
	u *stateless.WorkflowInfo,
	statuses map[api.JobUpdateStatus]struct{},
) (bool, error) {

	d, err := opaquedata.Deserialize(u.GetOpaqueData())
	if err != nil {
		return false, fmt.Errorf("deserialize opaque data: %s", err)
	}
	s, err := ptoa.NewJobUpdateStatus(u.GetStatus().GetState(), d)
	if err != nil {
		return false, fmt.Errorf("new job update status: %s", err)
	}
	_, ok := statuses[s]
	return ok, nil
}

// getJobID maps k to a job id.
//
// TODO: To be deprecated in favor of getJobSummaries.
// Aggregator expects job key environment to be set in response of
// GetJobUpdateDetails to filter by deployment_id. On filtering via job key
// role, original peloton job name is not known to set job key environment.
func (h *ServiceHandler) getJobID(
	ctx context.Context,
	k *api.JobKey,
) (*peloton.JobID, error) {
	req := &statelesssvc.GetJobIDFromJobNameRequest{
		JobName: atop.NewJobName(k),
	}
	resp, err := h.jobClient.GetJobIDFromJobName(ctx, req)
	if err != nil {
		return nil, err
	}
	// results are sorted chronologically, return the latest one
	return resp.GetJobId()[0], nil
}

// queryJobIDs takes optional job key components and returns the Peloton job ids
// which match the set parameters. E.g. queryJobIDs("myservice", "", "") will return
// job ids which match role=myservice.
func (h *ServiceHandler) queryJobIDs(
	ctx context.Context,
	role, env, name string,
) ([]*peloton.JobID, error) {

	if role != "" && env != "" && name != "" {
		// All job key components set, just use a job key query directly.
		id, err := h.getJobID(ctx, &api.JobKey{
			Role:        ptr.String(role),
			Environment: ptr.String(env),
			Name:        ptr.String(name),
		})
		if err != nil {
			return nil, err
		}
		return []*peloton.JobID{id}, nil
	}

	labels := label.BuildPartialAuroraJobKeyLabels(role, env, name)
	if len(labels) == 0 {
		// TODO(kevinxu): do we need to return all job ids in this case?
		return nil, errors.New("no filters set")
	}

	req := &statelesssvc.QueryJobsRequest{
		Spec: &stateless.QuerySpec{
			Labels: labels,
		},
	}
	resp, err := h.jobClient.QueryJobs(ctx, req)
	if err != nil {
		return nil, err
	}
	jobIDs := make([]*peloton.JobID, 0, len(resp.GetRecords()))
	for _, record := range resp.GetRecords() {
		jobIDs = append(jobIDs, record.GetJobId())
	}
	return jobIDs, nil
}

// getJobSummaries returns Peloton JobSummary based on Aurora JobKey passed in.
func (h *ServiceHandler) getJobSummaries(
	ctx context.Context,
	k *api.JobKey,
) ([]*stateless.JobSummary, error) {
	req := &statelesssvc.GetJobIDFromJobNameRequest{
		JobName: atop.NewJobName(k),
	}
	resp, err := h.jobClient.GetJobIDFromJobName(ctx, req)
	if err != nil {
		return nil, err
	}
	// results are sorted chronologically, return the latest one
	return []*stateless.JobSummary{
		{
			JobId: resp.GetJobId()[0],
			Name:  atop.NewJobName(k),
		},
	}, nil
}

// queryJobSummaries takes optional job key components and returns the Peloton
// job summaries which match the set parameters. E.g. queryJobSummaries("myservice", "", "")
// will return summaries which match role=myservice.
func (h *ServiceHandler) queryJobSummaries(
	ctx context.Context,
	role, env, name string,
) ([]*stateless.JobSummary, error) {
	req := &statelesssvc.QueryJobsRequest{
		Spec: &stateless.QuerySpec{
			Labels: label.BuildPartialAuroraJobKeyLabels(role, env, name),
		},
	}
	resp, err := h.jobClient.QueryJobs(ctx, req)
	if err != nil {
		return nil, err
	}
	return resp.GetRecords(), nil
}

// getJobIDsFromTaskQuery queries peloton job ids based on aurora TaskQuery.
// Note that it will not throw error when no job is found. The current
// behavior for querying:
// 1. If TaskQuery.JobKeys is present, the job keys there to query job ids
// 2. Otherwise use TaskQuery.Role, TaskQuery.Environment and
//    TaskQuery.JobName to construct a job key (those 3 fields may not be
//    all present), and use it to query job ids.
func (h *ServiceHandler) getJobIDsFromTaskQuery(
	ctx context.Context,
	query *api.TaskQuery,
) ([]*peloton.JobID, error) {
	if query == nil {
		return nil, errors.New("task query is nil")
	}

	// use job_keys to query if present
	if query.IsSetJobKeys() {
		var ids []*peloton.JobID
		for _, jobKey := range query.GetJobKeys() {
			id, err := h.getJobID(ctx, jobKey)
			if err != nil {
				if yarpcerrors.IsNotFound(err) {
					continue
				}
				return nil, errors.Wrapf(err, "get job id for %q", jobKey)
			}
			ids = append(ids, id)
		}
		return ids, nil
	}

	ids, err := h.queryJobIDs(
		ctx, query.GetRole(), query.GetEnvironment(), query.GetJobName())
	if err != nil {
		if yarpcerrors.IsNotFound(err) {
			// ignore not found error and return empty job ids
			return nil, nil
		}
		return nil, errors.Wrapf(err, "get job ids")
	}
	return ids, nil
}

func (h *ServiceHandler) getCurrentJobVersion(
	ctx context.Context,
	id *peloton.JobID,
) (*peloton.EntityVersion, error) {
	summary, err := h.getJobInfoSummary(ctx, id)
	if err != nil {
		return nil, err
	}
	return summary.GetStatus().GetVersion(), nil
}

// getJobInfo calls jobmgr to get JobInfo based on JobID.
func (h *ServiceHandler) getJobInfo(
	ctx context.Context,
	jobID *peloton.JobID,
) (*stateless.JobInfo, error) {
	req := &statelesssvc.GetJobRequest{
		JobId:       jobID,
		SummaryOnly: false,
	}
	resp, err := h.jobClient.GetJob(ctx, req)
	if err != nil {
		return nil, err
	}
	return resp.GetJobInfo(), nil
}

// getJobInfoSummary calls jobmgr to get JobSummary based on JobID.
func (h *ServiceHandler) getJobInfoSummary(
	ctx context.Context,
	jobID *peloton.JobID,
) (*stateless.JobSummary, error) {
	req := &statelesssvc.GetJobRequest{
		JobId:       jobID,
		SummaryOnly: true,
	}
	resp, err := h.jobClient.GetJob(ctx, req)
	if err != nil {
		return nil, err
	}
	return resp.GetSummary(), nil
}

func (h *ServiceHandler) getWorkflowInfo(
	ctx context.Context,
	jobID *peloton.JobID,
) (*stateless.WorkflowInfo, error) {

	req := &statelesssvc.GetJobRequest{
		JobId: jobID,
	}

	resp, err := h.jobClient.GetJob(ctx, req)
	if err != nil {
		return nil, err
	}

	return resp.GetWorkflowInfo(), nil
}

// queryPods calls jobmgr to query a list of PodInfo based on input JobID.
func (h *ServiceHandler) queryPods(
	ctx context.Context,
	jobID *peloton.JobID,
	states []pod.PodState,
	limit uint32,
) ([]*pod.PodInfo, error) {
	req := &statelesssvc.QueryPodsRequest{
		JobId: jobID,
		Spec: &pod.QuerySpec{
			PodStates: states,
			Pagination: &pbquery.PaginationSpec{
				Limit: limit,
			},
		},
		Pagination: &pbquery.PaginationSpec{
			Limit: limit,
		},
	}
	resp, err := h.jobClient.QueryPods(ctx, req)
	if err != nil {
		return nil, err
	}
	return resp.GetPods(), nil
}

// getPodEvents calls jobmgr to get a list of PodEvent based on PodName.
func (h *ServiceHandler) getPodEvents(
	ctx context.Context,
	podName *peloton.PodName,
) ([]*pod.PodEvent, error) {
	req := &podsvc.GetPodEventsRequest{
		PodName: podName,
	}
	resp, err := h.podClient.GetPodEvents(ctx, req)
	if err != nil {
		return nil, err
	}
	return resp.GetEvents(), nil
}
